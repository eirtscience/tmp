{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from googletrans import Translator\n",
    "import numpy as np\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPropertyAttribute(csv_file,list_sheet_name,translate=False,cols_name=\"F\"):\n",
    "    \n",
    "    translator = Translator()\n",
    "        \n",
    "    data=pd.read_excel(csv_file,sheet_name=list_sheet_name,usecols=cols_name,keep_default_na=False)\n",
    "    blackcreek_attr=[ attribute for sheet_name in data for col in data[sheet_name].columns for attribute in data[sheet_name][col].values if len(attribute) > 0]\n",
    "   \n",
    "    if translate:\n",
    "        origin_list=\"\\n\".join(blackcreek_attr)                  \n",
    "        translate=translator.translate(origin_list,dest='en')\n",
    "        blackcreek_attr=translate.text.split(\"\\n\")\n",
    "        translate=None\n",
    "    return blackcreek_attr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n",
      "['Name', 'Province', 'City Name', 'City Coordinates', 'Administrative Region', 'Administrative Region Description', 'Administrative Region Coordinates', 'Area', 'Area Description', 'Area Coordinates', 'Sub-District', 'Sub-District Description', 'Sub-District Coordinates', '*Administrative Region', '*Estate Name', '*Primary Category', 'Project Specialty', 'Area', 'Sub-District', 'Property Alias', 'Romanization Short Form', 'Property Name Full Romanization', 'Property Address', 'Property Deed Address', 'Advance Sale Address', 'Primary Building Type', 'Property Rights Form', 'Land Use', 'Parcel No.', 'Land Use Start Date', 'Land Use End Date', 'Land Use Term', 'Land Category', 'Capping Date', 'Completion Check and Acceptance Date', 'Public Sale Date', 'Construction Commencement Date', 'Occupation Date', 'Internal Sale Date', 'Longitude', 'Latitude', 'Site Area', 'Gross Floor Area', 'Saleable Area', 'Office Area', 'Commercial Area', 'Industrial Area', 'Other Purpose Area', 'Advance Sale Permit No.', 'Date Obtained Advance Sale Permit ', 'Advance Sale Scope', 'Total No. of Buildings', 'Total No. of Units', 'Estate Scale', 'Construction Completion Date', 'Average Project Unit Price', 'Average Public Sale Unit Price', 'Ring Location', 'Nearby Subway Line', 'Community Maturity', 'Transaction Frequency', 'Within Desirable School Zone?', 'Vacancy Ratio', 'Floor Area Ratio', 'Green Space Ratio', 'No. of Parking Spaces', 'Description for Parking Spaces', 'Developed by', 'Property Managed by', 'Management Fees', 'Contact for Property Management', 'Quality of Property Management', 'Facilities', 'Construction Quality', 'Interior Decoration Quality', 'Heating', 'East', 'West', 'South', 'North', 'Miscellaneous Comments on surroundings', 'General Comments on Property', 'Pricing Updated on', 'Appraising Coefficient', 'Building Coverage Ratio', 'Property Surveyed?', 'Basement Functions', 'Facilities Quality', 'Other Price-Positive Factors', 'Other Price-Negative Factors', 'Fundamental Data Completed?', 'Price Data Source', 'Other Miscellaneous Comments', 'Administrative Region', 'Estate Name', 'Building Name', 'Building Function', 'No. of Floors', 'Building Name during Advance Sale', 'Building Alias', 'Building Name on Building Signs', 'Building Name on Deed', 'Elevator available?', 'Structural Material(s)', 'Primary Building Type', 'Property Rights Form', 'Construction Completion Date', 'Occupation Date', 'Building Proximity to', 'Types of Units', 'Elevator Ratio', 'Includes School Place Allocation?', 'Average Building Unit Price', 'Average Advance Sale Price ', 'Advance Sale Permit No.', 'No. of Blocks', 'No. of Units per Floor', 'Total No. of Units', 'Average Ceiling Height', 'Gross Floor Area', 'Advance Sale Date', 'External Wall Material(s)', 'Type of Wall', 'Interior Decoration', 'State of Maintenance', 'Facilities', 'Piped Gas', 'Heating', 'No. of Elevators', 'Brand of Elevators', 'Floor Groupings', 'Podium Floors', 'Total Podium Floor Area', 'Total Tower Floor Area', 'No. of Residential Units', 'Total Residential Floor Area', 'No. of Non-Residential Units', 'Total Non-Residential Floor Area', 'No. of Basement Levels', 'Total Basement Floor Area', 'Basement Functions', 'Longitude', 'Latitude', 'Floor for Standardized Price ', 'Distance between Buildings', 'Private Garden?', 'Scenes Available', 'Primary Building Facing', 'Other Price-Positive Factors', 'Other Price-Negative Factors', 'Supplementary Buildings Unit Price', 'Appraising Coefficient', 'Other Miscellaneous Comments', 'Administrative Region', 'Estate Name', 'Building Name', 'Unit Name', 'Floor', 'Block', 'Actual Floor', 'Gross Floor Area', 'Floor Area Confirmed or Estimated?', 'Unit Floor Area (includes outdoor balconies)', 'Unit Function', 'Unit Type: # of Living Rooms', 'Unit Type: # of Dining Rooms', 'Unit Type: # of Rooms', 'Unit Type: # of Bathrooms', 'Unit Structure', 'Unit Primary Facing', 'Unit Scene Available', 'Ventilation and Natural Lighting Quality', 'Noise Level', 'Interior Decoration', 'Interior Partition', 'Type of Supplementary Building', 'Area of Supplementary Building', 'Includes Garden?', 'Unit Price (Price/Area)', 'Total Price (Price)', 'Includes Kitchen?', 'No. of Balconies', 'No. of Washrooms', 'Appraising Coefficient', 'Other Miscellaneous Comments', 'Name', 'Estate Name', 'Administrative Region', 'Building Name', 'Floor', 'Unit Name', 'Total Number of Floors', 'No. of Floors', 'Unit Function', 'Gross Floor Area', 'Unit Price (Price/Area)', 'Total Price (Price)', 'Case Type', 'Unit Primary Facing', 'Type of Building', 'Unit Type: # of Living Rooms', 'Unit Type: # of Dining Rooms', 'Unit Type: # of Rooms', 'Unit Type: # of Bathrooms', 'Unit Structure', 'Construction Completion Date', 'Interior Decoration', 'Useable Floor Area', 'Balance of Deed Term', 'Discount by Wear & Tear', 'Currency', 'Supplementary Building', 'Communal Facilities', 'Source of Case', 'Website of Case', 'Contact No. of Source', 'Miscellaneous', 'Reported Owner', 'Sale Contract Date', 'Sale Recording Date', 'Buyer', 'Seller', 'Sale Price', 'Sale Document Number', 'Other Properties in Sale', 'Mortgage Recording Date', 'Maturity Date', 'Loan Amount', 'Lender', 'Cross Collateralized Properties', 'Mortgage Document Number', 'Property Tax Payer Mailing Street Address', 'Property Tax Payer Mailing City', 'Property Tax Payer Mailing Province', 'Mortgage 1 Date', 'Mortgage 1 Amount', 'Mortgage 1 ACRIS ID', 'Mortgage 1 Lender Names', 'Total Debt with Buyer']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_file=\"data/prop_attr.xlsx\"\n",
    "\n",
    "list_sheet_name=[\"Black Creek\",\"Case List\"]\n",
    "\n",
    "blackcreek_attr=getPropertyAttribute(csv_file,list_sheet_name)\n",
    "\n",
    "print(len(blackcreek_attr))\n",
    "print(blackcreek_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackcreek_attr_translate=getPropertyAttribute(csv_file,list_sheet_name,translate=True,cols_name=\"D\")\n",
    "\n",
    "# print(len(blackcreek_attr_translate))\n",
    "# print(blackcreek_attr_translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-38-60a8d210db7e>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-60a8d210db7e>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    data.hist(['Serial No.', 'Admin. Region', 'Area Code'])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "csv_file=\"data/prop_data.xls\"\n",
    "\n",
    "def list_csv_data(csv_file):\n",
    "    return pd.read_excel(csv_file,header=1)\n",
    "\n",
    "    \n",
    "data=list_csv_data(csv_file)\n",
    "\n",
    "data.columns\n",
    "\n",
    "# data.count()\n",
    "\n",
    "# data.hist(['Serial No.', 'Admin. Region', 'Area Code'])\n",
    "\n",
    "# from pyspark.sql import *\n",
    "\n",
    "# spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "\n",
    "# row_df = Row(data)\n",
    "\n",
    "# df=spark.createDataFrame(row_df)\n",
    "\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=updateSchools, master=local[*]) created by __init__ at <ipython-input-10-01ac3476f248>:6 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-76072701751f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"updateSchools\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/blackcreek-data_mining/env/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/Project/blackcreek-data_mining/env/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=updateSchools, master=local[*]) created by __init__ at <ipython-input-10-01ac3476f248>:6 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import json\n",
    "sc = SparkContext()\n",
    "rdd = sc.parallelize([{'num': i} for i in xrange(10)])\n",
    "\n",
    "def remove__id(doc):\n",
    "    # `_id` field needs to be removed from the document\n",
    "    # to be indexed, else configure this in `conf` while\n",
    "    # calling the `saveAsNewAPIHadoopFile` API\n",
    "    doc.pop('_id', '')\n",
    "    return doc\n",
    "\n",
    "new_rdd = rdd.map(remove__id).map(json.dumps).map(lambda x: ('key', x))\n",
    "\n",
    "new_rdd.saveAsNewAPIHadoopFile(\n",
    "    path='-',\n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf={\n",
    "        \"es.nodes\" : 'localhost',\n",
    "        \"es.port\" : '9200',\n",
    "        \"es.resource\" : '%s/%s' % ('index_name', 'doc_type_name'),\n",
    "        \"es.input.json\": 'true'\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"school\"] == \"Harvard\").show()\n",
    "\n",
    "r=df.rdd.collect()\n",
    "id = r[0][1]['_id']\n",
    "\n",
    "df2=df.withColumn(\"_id\", lit(id)) \n",
    "\n",
    "esconf={}\n",
    "esconf[\"es.mapping.id\"] = \"_id\"\n",
    "esconf[\"es.nodes\"] = \"localhost\"\n",
    "esconf[\"es.port\"] = \"9200\"\n",
    "esconf[\"es.update.script.inline\"] = \"ctx._source.location = params.location\"\n",
    "esconf[\"es.update.script.params\"] = \"location:\"\n",
    "esconf[\"es.write.operation\"] = \"upsert\"\n",
    "\n",
    "df2.write.format(\"org.elasticsearch.spark.sql\").options(**esconf).mode(\"append\").save(\"school/info\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackcreek-data-mining",
   "language": "python",
   "name": "blackcreek-data-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
